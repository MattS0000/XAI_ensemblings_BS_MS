{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99da430b-cdf0-40df-a5b9-303fa424ce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import captum\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "from os import path\n",
    "from skimage import io\n",
    "from torch import optim\n",
    "from torch.nn import Linear, CrossEntropyLoss\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa9f5cb2-a3ad-4e96-8829-6716fc829f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_frame = pd.read_csv(path.join('..', 'input', 'inz-data-prep', 'easy_labels_and_data.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4389953-c622-4b13-a3e2-c36631f86e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_bbox(image, bboxes: pd.DataFrame):\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    for idx, bbox in bboxes.iterrows():\n",
    "        x = (bbox[0] - bbox[2] / 2) * 512\n",
    "        y = (bbox[1] - bbox[3] / 2) * 512\n",
    "\n",
    "        rect = Rectangle((x, y), bbox[2] * 512, bbox[3] * 512, fill=False, edgecolor='r')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "class CropWeedDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, labels_csv, images_dir, transform=None):\n",
    "        self.labels_frame = pd.read_csv(labels_csv, index_col=0)\n",
    "        self.grouped_labels_frame = self.labels_frame.groupby('filename').count()\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.grouped_labels_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = path.join(self.images_dir,\n",
    "                                self.grouped_labels_frame.iloc[idx].name)\n",
    "        image = io.imread(img_name)\n",
    "        label = int(self.grouped_labels_frame.iloc[idx, 0] > 0)\n",
    "        \n",
    "        sample = {'image': image, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def get_bbox(self, idx):\n",
    "        bbox = self.labels_frame.loc[self.labels_frame['filename'] == self.grouped_labels_frame.iloc[idx].name].iloc[:,2:6]\n",
    "        return bbox\n",
    "    \n",
    "    def show_image(self, idx):\n",
    "        sample = self[idx]\n",
    "        bbox = self.get_bbox(idx)\n",
    "\n",
    "        if self.transform:\n",
    "            numpy_image = sample['image'].numpy().transpose(1, 2, 0)\n",
    "        else:\n",
    "            numpy_image = sample['image']\n",
    "\n",
    "        show_image_bbox(numpy_image, bbox)\n",
    "\n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image) / 255,\n",
    "                'label': torch.asarray(label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "097fe7f7-90fb-4c17-abe2-55a7e1ab2133",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CropWeedDataset(labels_csv=path.join('..', 'input', 'inz-data-prep', 'easy_labels_and_data.csv'),\n",
    "                          images_dir=path.join('..','input','crop-and-weed-detection-data-with-bounding-boxes','agri_data', 'data'),\n",
    "                          transform=ToTensor())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eb10183-519a-4caa-a9b6-f006a73b7c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = random_split(dataset, [950, 204], generator=torch.Generator().manual_seed(420))\n",
    "dataloaders = {'train':DataLoader(train, batch_size=8, shuffle=True, num_workers=2), \n",
    "               'val': DataLoader(test, batch_size=8, shuffle=True, num_workers=2)}\n",
    "dataset_sizes = {'train': 950,\n",
    "                'val': 204}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850136c-e958-47ee-abb6-a5c143c4ef1f",
   "metadata": {},
   "source": [
    "\n",
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcbeb23b-85e1-4acb-b011-c224d7460de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18()\n",
    "model.fc = Linear(512, 2)\n",
    "model.load_state_dict(torch.load(path.join('models', 'resnet-18'), map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "007ebcd4-fce6-48a8-a94b-1b5ca0191b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7695,  2.8448]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.unsqueeze(dataset[0]['image'], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "494beff6-7572-43b7-9dd8-b90988120d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_func(input):\n",
    "    return input * 0\n",
    "\n",
    "from torchvision import transforms\n",
    "normalize = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7259be35-3fd7-46bc-8e8f-21f9de26e712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.insights import AttributionVisualizer, Batch\n",
    "from captum.insights.attr_vis.features import ImageFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30b64238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_data_iter():\n",
    "    dataset = torch.utils.data.Subset(CropWeedDataset(labels_csv=path.join('..', 'input', 'inz-data-prep', 'easy_labels_and_data.csv'),\n",
    "                          images_dir=path.join('..','input','crop-and-weed-detection-data-with-bounding-boxes','agri_data', 'data'),\n",
    "                          transform=ToTensor()), range(3))\n",
    "    dataloader = iter(\n",
    "        torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "    )\n",
    "    while True:\n",
    "        sample_dict = next(dataloader)\n",
    "        images = sample_dict['image']\n",
    "        labels = sample_dict['label']\n",
    "        yield Batch(inputs=images, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34aad790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x251d0487040>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.utils.data.Subset(dataset, range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f7a1cc5-7315-4016-9b9b-85f724c82925",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = AttributionVisualizer(models=[model],\n",
    "                      classes=['crop', 'weed'],\n",
    "                      features=[ImageFeature('photo',baseline_transforms=[baseline_func], input_transforms=[normalize])],\n",
    "                      dataset=formatted_data_iter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45374c79-bafc-4e8f-8da1-0d9142e09726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25d1982fa534411abcdcd39b99225bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CaptumInsights(insights_config={'classes': ['crop', 'weed'], 'methods': ['Deconvolution', 'Deep Lift', 'Guidedâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755e8e06eda843b282cdf18ea792c735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualizer.render(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc6e38-e120-4f6f-a2a8-a38e0e44a055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
